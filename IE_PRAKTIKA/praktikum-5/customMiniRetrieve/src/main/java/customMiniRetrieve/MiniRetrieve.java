/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package customMiniRetrieve;

import java.io.*;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.sql.Array;
import java.util.*;
import java.util.stream.Collectors;
import java.util.stream.Stream;

public class MiniRetrieve {
    private static final String DOCUMENT_PATH = "documents/";
    private static final String QUERRY_PATH = "queries/";

    private static Map<Integer, File> myQueries;
    private static Map<Integer, File> myDocuments;
    //Indexes
    private static final Map<String, Map<File, Integer>> invIndex = new HashMap<>();
    private static final Map<File, Map<String,Integer>> nonInvIndex = new HashMap<>();
    private static final Map<File, Map<String, Integer>> querryIndex = new HashMap<>();
    //MiniRetrive components
    private static Map<File,Double> accumulator;
    private static Map<File,Double> dNorm = new HashMap<>();
    private static Map<String,Double> idf = new HashMap<>();
    private static double qNorm = 0.0;
    private static double totalNumberOfDocuments = 0.0;

    public static void main(String[] args) {
        myQueries = readDirectory(QUERRY_PATH);
        myDocuments = readDirectory(DOCUMENT_PATH);
        //System.out.println(tokenizeString(querries.get(1)));
        createNonAndInvIndex(myDocuments);
        totalNumberOfDocuments = nonInvIndex.size();
        createQuerryIndex(myQueries);
        createIDFsAndDocNormalizers(nonInvIndex);
        processQueries(myQueries);
    }

    private static void processQueries(Map<Integer, File> queries){
        for (File querie : queries.values()) {
            qNorm = 0.0;
            accumulator = new LinkedHashMap<>();
            for (String term : tokenizeString(readFile(querie))) {
                if(!idf.containsKey(term)){
                    idf.put(term,Math.log(1.0+totalNumberOfDocuments));
                }
                double b = querryIndex.get(querie).get(term) * idf.get(term);
                qNorm += (b*b);
                if(invIndex.containsKey(term)){
                    for (File doc : invIndex.get(term).keySet()) {
                        double a = invIndex.get(term).get(doc) * idf.get(term);
                        if(accumulator.containsKey(doc)) {
                            double accuValue = accumulator.get(doc);
                            accuValue += a*b;
                            accumulator.put(doc, accuValue);
                        }
                        else accumulator.put(doc, a*b);
                    }
                }
            }

            //2e Teil hier
            qNorm = Math.sqrt(qNorm);
            for (File doc : accumulator.keySet()) {
                //normalize lenght of vectors
                accumulator.put(doc, (accumulator.get(doc)*1000/(dNorm.get(doc)*qNorm)));
            }
            List<Map.Entry<File, Double>> results = sortLinkedMap(accumulator);
            for(int i =0; i < 10 ; i++){
                System.out.println(querie.toString() + " Q0 " + results.get(results.size()-i-1).getKey()
                        +"   "+ results.get(results.size()-i-1).getValue()+ " miniretrive");
            }
        }
    }

    /**
     *
     * @param map to be sorted by value attribute
     * @return the Document with the highest double rating
     */
    private static  List<Map.Entry<File, Double>> sortLinkedMap(Map<File,Double> map){
        List<Map.Entry<File, Double>> entries =
                new ArrayList<Map.Entry<File, Double>>(map.entrySet());
        Collections.sort(entries, new Comparator<Map.Entry<File, Double>>() {
            public int compare(Map.Entry<File, Double> a, Map.Entry<File, Double> b){
                return a.getValue().compareTo(b.getValue());
            }
        });
        return entries;
    }


    private static void createIDFsAndDocNormalizers(Map<File, Map<String,Integer>> nonInvIndex){
        for (File doc : nonInvIndex.keySet()) {
            dNorm.put(doc,0.0);
            for(String term : tokenizeString(readFile(doc))){
                //log((1+totalNumberOfDocuments)/(1+documentFrequency))
                double IDFvalue = Math.log((1.0+totalNumberOfDocuments)/(1.0+invIndex.get(term).size()));
                idf.put(term,IDFvalue);
                double a = nonInvIndex.get(doc).get(term)* idf.get(term);
                dNorm.put(doc,dNorm.get(doc)+a*a);
            }
            dNorm.put(doc,Math.sqrt(dNorm.get(doc)));
        }
    }

    private static void createQuerryIndex(Map<Integer, File> querries){
        for (File querry : querries.values()) {
            List<String> terms = tokenizeString(readFile(querry));
            for (String term : terms) {
                //if nonInvIndex already contains a document and a term already containing that term one then increment
                if(querryIndex.containsKey(querry) && querryIndex.get(querry).containsKey(term)){
                    Map<String,Integer> mapofDocument = querryIndex.get(querry);
                    mapofDocument.put(term,(mapofDocument.get(term)+1));
                    // if nonInvIndex contains document but not that term then put term in doc with count1
                }else if(querryIndex.containsKey(querry)){
                    querryIndex.get(querry).put(term,1);
                    // if doc is not in nonInvIndex then put it with the term it corresponds to
                }else{
                    Map<String,Integer> newMap = new HashMap<>();
                    newMap.put(term,1);
                    querryIndex.put(querry, newMap);
                }
            }
        }

    }

    private static void createNonAndInvIndex( Map<Integer, File> documents) {
        for (File doc : documents.values()) {
            List<String> terms = tokenizeString(readFile(doc));
            for (String term : terms) {
                //if invIndex already contains word and a doc already containing that term once then increment
                if (invIndex.containsKey(term) && invIndex.get(term).containsKey(doc)) {
                    Map<File, Integer> mapOfTerm = invIndex.get(term);
                    mapOfTerm.put(doc, (mapOfTerm.get(doc) + 1));
                // if invIndex contains word already but not with this doc put doc with count 1
                } else if (invIndex.containsKey(term)) {
                    invIndex.get(term).put(doc, 1);
                // if term is new in invIndex put it there with the doc it was found
                } else {
                    Map<File,Integer> newMap = new HashMap<>();
                    newMap.put(doc, 1);
                    invIndex.put(term, newMap);
                }
                //if nonInvIndex already contains a document and a term already containing that term one then increment
                if(nonInvIndex.containsKey(doc) && nonInvIndex.get(doc).containsKey(term)){
                    Map<String,Integer> mapofDocument = nonInvIndex.get(doc);
                    mapofDocument.put(term,(mapofDocument.get(term)+1));
                // if nonInvIndex contains document but not that term then put term in doc with count1
                }else if(nonInvIndex.containsKey(doc)){
                    nonInvIndex.get(doc).put(term,1);
                // if doc is not in nonInvIndex then put it with the term it corresponds to
                }else{
                    Map<String,Integer> newMap = new HashMap<>();
                    newMap.put(term,1);
                    nonInvIndex.put(doc, newMap);
                }
            }
        }
    }

    /**
     * Splits a string into individual words, trims spaces of, removes punctuation and maps to lowercase.
     * @param fullString String to be tokenized
     * @return List containing all words of String.
     */
    private static List<String> tokenizeString(String fullString) {
        List<String> rawTokens = Arrays.asList(fullString.split("\\W+"));
        List<String> trimmedTokens = rawTokens.stream().map(String::trim).collect(Collectors.toList());
        //List<String> cleanedTokens = trimmedTokens.stream().map(s -> s.replace("\\W+", "")).collect(Collectors.toList());
        List<String> lowercasedToken = trimmedTokens.stream().map(String::toLowerCase).collect(Collectors.toList());
        return lowercasedToken;
    }

    /**
     * Reas a file and returns the content as a string;
     * @param file File
     * @return Contents of file as String
     */
    private static String readFile(File file) {
            String output = "";
            try (BufferedReader bf = new BufferedReader(new FileReader(file))) {
                String line = null;
                StringBuilder sb = new StringBuilder();
                while ((line = bf.readLine()) != null) {
                    sb.append(line);
                }
                output = sb.toString();
            } catch (FileNotFoundException e) {
                e.printStackTrace();
            } catch (IOException e) {
                e.printStackTrace();
            }
        return output;
    }

    /**
     * Reads a direcotry and returns all files and their id as Key
     * @param path path to directory ALL FILE NAMES MUST BE NUMBERS!
     * @return Map wit Files as value and their ID as name
     */
    private static Map<Integer, File> readDirectory(String path) {
        HashMap<Integer, File> map = new HashMap<>();
        try (Stream<Path> paths = Files.walk(Paths.get(path))) {
            paths
                    .filter(Files::isRegularFile)
                    .forEach(e -> map.put(Integer.valueOf(e.toFile().getName()), e.toFile()));
        } catch (IOException e) {
            e.printStackTrace();
        }
        return map;
    }
}
